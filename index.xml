<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>int0x80</title>
    <link>https://int0x80.github.io/blog/index.xml</link>
    <description>Recent content on int0x80</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Jan 2017 10:47:27 -0600</lastBuildDate>
    <atom:link href="https://int0x80.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Home Lab: ESXi</title>
      <link>https://int0x80.github.io/blog/post/home-lab-esxi/</link>
      <pubDate>Wed, 25 Jan 2017 10:47:27 -0600</pubDate>
      
      <guid>https://int0x80.github.io/blog/post/home-lab-esxi/</guid>
      <description>&lt;p&gt;This is the final post in the three-part series covering my lab at home (&lt;a href=&#34;https://int0x80.github.io/blog/post/home-lab-overview/&#34;&gt;Part 1&lt;/a&gt;, &lt;a href=&#34;https://int0x80.github.io/blog/post/home-lab-san/&#34;&gt;Part 2&lt;/a&gt;).  The ESXi node runs vSphere 6 sporting two Xeon X5670 CPUs and 96GB of RAM.  Storage is accessed via iSCSI over a 10Gbps link to the SAN.&lt;/p&gt;

&lt;p&gt;This build was my first time working with ESXi.  Initially, I had all of my storage in the ESXi system before deciding to build a SAN.  Eventually the PSU in the ESXi node died, which prompted me to replace the PSU and commit to the SAN.  Other than that, the system runs great with plenty of resources for running many VMs concurrently. &lt;/p&gt;

&lt;h1 id=&#34;hardware&#34;&gt;Hardware&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://h20564.www2.hp.com/hpsc/doc/public/display?docId=emr_na-c01709724&#34;&gt;HP z800, 2x Xeon X5670, 96GB RAM &lt;/a&gt;: $1400.00&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ebay.com/itm/131999754708&#34;&gt;HP z800 850W Power Supply&lt;/a&gt;: $95.99&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Total:&lt;/strong&gt; $1495.99&lt;/p&gt;

&lt;p&gt;The intent was to re-purpose the drives in my old server to act as the storage for the new lab server.  The old server was primarily a file server running on old hardware and didn&amp;rsquo;t have the resources to serve up VMs.  Unfortunately, two of the drives failed after the initial order of the workstation.  This was also a factor in deciding to build a SAN.&lt;/p&gt;

&lt;h1 id=&#34;install-esxi-to-usb&#34;&gt;Install ESXi to USB&lt;/h1&gt;

&lt;p&gt;Register a free account at &lt;a href=&#34;https://vmware.com&#34;&gt;vmware.com&lt;/a&gt; and use the account to download a trial for vSphere 6.0 and vSphere Client.&lt;/p&gt;

&lt;p&gt;This is likely overkill but I was unfamiliar with this line of VMware products when I started.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ESXi:&lt;/strong&gt; The hypervisor running on each system with VMs.  Also known as vSphere.  It is free and simply requires a license serial provided by vmware.com with a free account there.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;vSphere Client:&lt;/strong&gt; Windows application that interfaces with the ESXi server.  Create and manipulate VMs with this.  Also free.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Boot the vSphere iso in a VM and attach a FAT32 formatted USB flash drive to the VM.  This is where ESXi will live so use a flash drive with at least 8GB of space that will not be missed.  Select the USB drive as the target install drive in the ESXi setup process.  Finish the install and the workstation can now boot ESXi from the USB drive.&lt;/p&gt;

&lt;h1 id=&#34;connect-to-san-via-iscsi&#34;&gt;Connect to SAN via iSCSI&lt;/h1&gt;

&lt;p&gt;The first part of the ESXi setup is attaching to the storage in the SAN so that OS images and VMs have somewhere to live.&lt;/p&gt;

&lt;h2 id=&#34;add-network-adapter&#34;&gt;Add Network Adapter&lt;/h2&gt;

&lt;p&gt;In vSphere Client, select the &lt;strong&gt;Configuration&lt;/strong&gt; tab at the top followed by the &lt;strong&gt;Networking&lt;/strong&gt; link on the left.  Next click &lt;strong&gt;Add Networking&amp;hellip;&lt;/strong&gt;.  I completed the setup process with the following details.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Connection Type:&lt;/strong&gt; VMkernel&lt;/li&gt;
&lt;li&gt;Select the dedicated 10Gbps NIC&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network Label:&lt;/strong&gt; iSCSI&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IP Address:&lt;/strong&gt; &lt;code&gt;10.11.1.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subnet Mask:&lt;/strong&gt; &lt;code&gt;255.255.255.0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This produces the resulting interface.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/1061032/22218566/4afbb60a-e15c-11e6-8534-0a34a866c2ea.png&#34; alt=&#34;ESXi iSCSI Network Adapter&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;add-storage-adapter&#34;&gt;Add Storage Adapter&lt;/h2&gt;

&lt;p&gt;In vSphere Client, select the &lt;strong&gt;Configuration&lt;/strong&gt; tab at the top followed by the &lt;strong&gt;Storage Adapters&lt;/strong&gt; link on the left.  Next click &lt;strong&gt;Add&amp;hellip;&lt;/strong&gt;, and select &lt;strong&gt;Add Software iSCSI Adapter&lt;/strong&gt;.  This produces the resulting adapter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/1061032/22219087/3e356b80-e15e-11e6-8685-3850e609abe9.png&#34; alt=&#34;ESXi iSCSI Storage Adapter&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;storage-adapter-configuration&#34;&gt;Storage Adapter Configuration&lt;/h2&gt;

&lt;p&gt;Still in the &lt;strong&gt;Storage Adapters&lt;/strong&gt; window, click &lt;strong&gt;Properties&amp;hellip;&lt;/strong&gt;.  The &lt;strong&gt;General&lt;/strong&gt; tab should show &lt;code&gt;Status: Enabled&lt;/code&gt;, or you can enable it via the &lt;strong&gt;Configure&amp;hellip;&lt;/strong&gt; button.&lt;/p&gt;

&lt;p&gt;Under the &lt;strong&gt;Network Configuration&lt;/strong&gt; tab, click &lt;strong&gt;Add&amp;hellip;&lt;/strong&gt; and select the iSCSI network adapter added in the previous section.&lt;/p&gt;

&lt;p&gt;Under the &lt;strong&gt;Dynamic Discovery&lt;/strong&gt; tab, click &lt;strong&gt;Add&amp;hellip;&lt;/strong&gt; and enter the IP address and port of the SAN.  In my case &lt;code&gt;10.11.1.1&lt;/code&gt; and &lt;code&gt;3260&lt;/code&gt;, respectively.&lt;/p&gt;

&lt;p&gt;Allow ESXi to rescan the adapter, and you should see the resulting target now exposed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/1061032/22211254/5b317232-e141-11e6-9e1b-e0d8fc2aa3e4.png&#34; alt=&#34;ESXi iSCSI adapter&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;create-datastore&#34;&gt;Create Datastore&lt;/h1&gt;

&lt;p&gt;We now have storage available; ESXi just needs to be allowed access.  Select the &lt;strong&gt;Configuration&lt;/strong&gt; tab at the top followed by the &lt;strong&gt;Storage&lt;/strong&gt; link on the left.  Next click &lt;strong&gt;Add Storage&amp;hellip;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Disk/LUN&lt;/strong&gt; option is default, then select your iSCSI target on the next screen.  The remaining defaults should all be fine, then name your datastore.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/1061032/22219647/7063e6ca-e160-11e6-95b5-5c9bc412a366.png&#34; alt=&#34;ESXi Datastore&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Files can be uploaded via the &lt;code&gt;Browse Datastore...&lt;/code&gt; menu option from right-clicking the store.&lt;/p&gt;

&lt;h1 id=&#34;wrapping-up&#34;&gt;Wrapping Up&lt;/h1&gt;

&lt;p&gt;Finally the lab is setup and functional.  There is adequate storage, compute power, and memory to facilitate plenty of VMs running concurrently.  Systems and networks can quickly be spun up, used, and destroyed all with minimal overhead.  Now to hack things!&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;ps-pre-san-notes&#34;&gt;PS: Pre-SAN Notes&lt;/h1&gt;

&lt;p&gt;Here are my notes on building everything in the z800 without the SAN.  Some of the referenced hardware components like the RAID card, etc are enumerated in the SAN write-up.&lt;/p&gt;

&lt;h2 id=&#34;mini-sas-cable&#34;&gt;Mini SAS Cable&lt;/h2&gt;

&lt;p&gt;The z800 has a bay with 4 trays for the hard drives that attach to SATA connectors with cables already run to the motherboard. The SATA connectors in the hard drive bay are attached to a separate device, not a standard SATA cable that can be unplugged and swapped out.  This left me stuck with the existing female SATA connectors coming from the bay.  Standard SAS-SATA break outs are SFF-8087 cables which also terminate in female SATA connectors.  In order to connect the four drives in the bay, I needed a different break out.  &lt;a href=&#34;http://www.amazon.com/dp/B00WYA4SWU/&#34;&gt;This cable&lt;/a&gt; was $45 on Amazon, but I found it for $35 + $6.65 S&amp;amp;H on eBay.&lt;/p&gt;

&lt;h2 id=&#34;hard-drive-expansion-bays&#34;&gt;Hard Drive Expansion Bays&lt;/h2&gt;

&lt;p&gt;The tool-free design of the z800 case leaves blank expansion bays on the face.  In order to fit additional hard drives, I purchased two &lt;a href=&#34;https://www.amazon.com/dp/B00126U0VA/&#34;&gt;Kingwin SATA Bay&lt;/a&gt; trays.  These fit right in and connected directly to the Areca 1880i RAID card via SFF-8087 cable.&lt;/p&gt;

&lt;h2 id=&#34;areca-arc-1880i&#34;&gt;Areca ARC-1880i&lt;/h2&gt;

&lt;p&gt;Utilizing the RAID had two main aspects:  initializing the RAID with the correct parameters, and installing the driver to ESXi.&lt;/p&gt;

&lt;h3 id=&#34;raid-initialization&#34;&gt;RAID Initialization&lt;/h3&gt;

&lt;p&gt;With 6 drives at 6TB each, I went with a RAID6 to expose 24TB of storage.  ESXi does not work with the 4k blocks so the only option was to use the 64-bit LBA blocks for the RAID.  I selected a stripe size of 64k and waited ~9 hours for the RAID to initialize.&lt;/p&gt;

&lt;p&gt;If you want to do any quick tests with a RAID, simply build a RAID0 and it will be ready immediately.&lt;/p&gt;

&lt;h3 id=&#34;driver-install&#34;&gt;Driver Install&lt;/h3&gt;

&lt;p&gt;ESXi does not recognize the 1880i by default.  Grab the &lt;a href=&#34;http://www.areca.com.tw/support/s_vmware/vmware.htm&#34;&gt;vmware driver from Areca&lt;/a&gt; and extract the vib file.  Enable the ESXi Shell and SSH via console on the ESXi host under &lt;code&gt;Troubleshooting Options&lt;/code&gt; in the &lt;strong&gt;System Customization&lt;/strong&gt; menu.  Next, scp the vib file to /tmp/ on the ESXi host using the &lt;code&gt;root&lt;/code&gt; account previously configured.  Lastly, ssh into the root account on the ESXi host and run the following command replacing &lt;code&gt;your.vib&lt;/code&gt; with the correct filename:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# from the ESXi node
[root@esxi:~] esxcli software vib install -v /tmp/&amp;lt;your.vib&amp;gt; --no-sig-check
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reboot the server and the RAID should now show up in vSphere Client.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Home Lab: SAN</title>
      <link>https://int0x80.github.io/blog/post/home-lab-san/</link>
      <pubDate>Tue, 24 Jan 2017 12:45:19 -0600</pubDate>
      
      <guid>https://int0x80.github.io/blog/post/home-lab-san/</guid>
      <description>&lt;p&gt;This post is the second in a three-part series covering my lab at home (&lt;a href=&#34;https://int0x80.github.io/blog/post/home-lab-overview/&#34;&gt;Part 1&lt;/a&gt;, &lt;a href=&#34;https://int0x80.github.io/blog/post/home-lab-esxi/&#34;&gt;Part 3&lt;/a&gt;).  The SAN runs Ubuntu Server 16.04 and exposes ~22TB of storage via 10Gbps link.  The ESXi node then utilizes that storage via iSCSI.&lt;/p&gt;

&lt;p&gt;This build had a lot of first-time experiences for me.  My first SAN, my first hardware RAID, and my first exposure to iSCSI.  Some limitations of the build are already visible, along with some ways I could have saved money, but I am pretty happy with the current setup and will address growing pains as they arrive.&lt;/p&gt;

&lt;h1 id=&#34;hardware&#34;&gt;Hardware&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ebay.com/itm/252519213489&#34;&gt;ASRock 970 Extreme3 R1.01 Socket AM3+ Motherboard&lt;/a&gt;: $44.99&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/B0071RSMKA&#34;&gt;CPU Fan Bracket Base for AM3 socket&lt;/a&gt;: $5.49&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ebay.com/itm/222305768852&#34;&gt;16GB PC3-14900E DDR3 ECC RAM&lt;/a&gt;: $100.00&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ebay.com/itm/350983607686&#34;&gt;MELLANOX CONNECTX-2 PCIe 10Gbps ETHERNET NIC&lt;/a&gt; (2x): $34.60&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.fs.com/products/21269.html&#34;&gt;10G SFP+ Passive Direct Attach Copper Twinax Cable 30AWG&lt;/a&gt;: $9.50&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.areca.com.tw/products/1880.htm&#34;&gt;Areca ARC-1880i&lt;/a&gt;: $149.95&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/B005E2XTO8&#34;&gt;Mini SAS to SATA Breakout Cable&lt;/a&gt; (2x): $36.52&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/B01HXYRJYK&#34;&gt;Seasonic 650W 80 PLUS Titanium PSU&lt;/a&gt;: $173.19&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/B0091IZ1ZG&#34;&gt;Rosewill 4U Server Chassis (RSV-L4500)&lt;/a&gt;: $104.99&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.newegg.com/Product/Product.aspx?Item=N82E16820178966&#34;&gt;PNY CS1311 2.5&amp;rdquo; 120GB SSD&lt;/a&gt;: $49.99&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.amazon.com/dp/B0143BOR5A&#34;&gt;WD Red 6TB Pro&lt;/a&gt; (6x): $1782.92&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Total:&lt;/strong&gt; $2492.14&lt;/p&gt;

&lt;p&gt;Management of the system is done via SSH through the onboard NIC, while storage is exposed via iSCSI using the 10Gbps NIC.&lt;/p&gt;

&lt;p&gt;Some of the hardware components might seem like odd selections or at least a bit outdated.  Since this was my first build and I just wanted to get something up and running with minimal headaches, I based my build off of &lt;a href=&#34;https://thehomeserverblog.com/esxi-storage/building-a-homemade-san-on-the-cheap-32tb-for-1500/&#34;&gt;this whitebox build&lt;/a&gt; I found.  Everything runs fine so no real regrets thus far.  Let&amp;rsquo;s look more at the configuration and setup.&lt;/p&gt;

&lt;h1 id=&#34;areca-arc-1880i&#34;&gt;Areca ARC-1880i&lt;/h1&gt;

&lt;p&gt;The main step in setting up the storage was initializing the RAID with the correct parameters.  With 6 drives at 6TB each, I went with a RAID6 to expose ~22TB of storage.  I wanted to use 4k blocks but ran into some issues on ESXi, so the only option was to use the 64-bit LBA blocks for the RAID.  I selected a stripe size of 64k and waited ~9 hours for the RAID to initialize.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you want to do any quick tests with a RAID, simply build a RAID0 and it will be ready immediately.&lt;/p&gt;

&lt;h1 id=&#34;video-card&#34;&gt;Video Card&lt;/h1&gt;

&lt;p&gt;One of the limitations I encountered early was that the system wanted a video card to boot.  Most cards are PCI-e now, but the PCI-e slots on the board were taken by the 10Gbps NIC and the RAID card.  Fortunately, &lt;a href=&#34;https://twitter.com/indiecom&#34;&gt;jgor&lt;/a&gt; gave me an old PCI video card that he had.  There was some tom-foolery with getting the UEFI screen to display initially, but eventually things started working.  Keep this in mind if you decide to mirror my build.&lt;/p&gt;

&lt;h1 id=&#34;10gbps-twinax-cable&#34;&gt;10Gbps Twinax Cable&lt;/h1&gt;

&lt;p&gt;The 10Gbps cable linked above is about 18&amp;rdquo; in length.  This means the connected 10Gbps NICs need to be in very close proximity.  It shouldn&amp;rsquo;t be an issue for systems racked adjacent to each other, but in any other case you might consider a longer cable.&lt;/p&gt;

&lt;h1 id=&#34;iscsi&#34;&gt;iSCSI&lt;/h1&gt;

&lt;p&gt;After situating the hardware, I installed Ubuntu Server to the SSD.  Initially, I went with 16.10 as it was the most recent release, but encountered some issues and downgraded to 16.04.&lt;/p&gt;

&lt;p&gt;Installing the necessary packages was simple.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt update &amp;amp;&amp;amp; sudo apt dist-upgrade -y
$ sudo apt install -y iscsitarget iscsitarget-dkms
$ sudo sed -i -e &amp;quot;s/ISCSITARGET_ENABLE=false/ISCSITARGET_ENABLE=true/&amp;quot; /etc/default/iscsitarget
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last steps are to manually add the LUN, which is essentially a label applied to the storage interface.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo ietadm --op new --tid=3 --params Name=iqn.2014-11.home.lab.san:storage03  
$ sudo ietadm --op new --tid=3 --lun=0 --params Path=/dev/sda1,Type=blockio  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll cover the ESXi side of things in the next post, but the iSCSI adapter should now see the target storage available.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/1061032/22211254/5b317232-e141-11e6-9e1b-e0d8fc2aa3e4.png&#34; alt=&#34;ESXi iSCSI adapter&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;further-testing&#34;&gt;Further Testing&lt;/h2&gt;

&lt;p&gt;If you want to do some quick tests, or just mess around with iSCSI some more, there are some simple processes you can explore.  Some quick notes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ESXi 10Gbps NIC: &lt;code&gt;10.11.1.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;SAN 10Gbps NIC: &lt;code&gt;10.11.1.1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;SAN Management NIC: &lt;code&gt;192.168.1.5&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;verify-the-iscsi-service-is-listening-and-reachable&#34;&gt;Verify the iSCSI service is listening and reachable&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# from the ESXi node
[root@esxi:~] nc -z -s 10.11.1.2 10.11.1.1 3260  
Connection to 10.11.1.1 3260 port [tcp/*] succeeded!
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;add-a-small-iscsi-target-on-the-san&#34;&gt;Add a small iSCSI target on the SAN&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# on the SAN, 192.168.1.5
$ sudo dd if=/dev/zero of=/tmp/fileio.img bs=1 count=0 seek=2G  
$ ls -lh /tmp/fileio.img  
-rw-r--r-- 1 root root 2.0G Jan  3 11:45 /tmp/fileio.img  
  
$ sudo grep -v &amp;quot;^\s*#&amp;quot; /etc/iet/ietd.conf | grep -v &amp;quot;^$&amp;quot;  
Target iqn.2014-11.home.lab.san:storage01  
  Lun 0 Path=/tmp/fileio.img,Type=fileio  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;look-for-targets-via-the-management-interface&#34;&gt;Look for targets via the management interface&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# from a test system, 192.168.1.134
$ sudo iscsiadm -m discovery -t st -p 192.168.1.5  
192.168.1.5:3260,1 iqn.2014-11.home.lab.san:storage01  
10.11.1.1:3260,1 iqn.2014-11.home.lab.san:storage01  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;login-from-the-management-interface&#34;&gt;Login from the management interface&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# from a test system, 192.168.1.134
$ sudo iscsiadm -m node --login  
Logging in to [iface: default, target: iqn.2014-11.home.lab.san:storage01, portal: 10.11.1.1,3260] (multiple)  
Logging in to [iface: default, target: iqn.2014-11.home.lab.san:storage01, portal: 192.168.1.5,3260] (multiple)  
iscsiadm: Could not login to [iface: default, target: iqn.2014-11.home.lab.san:storage01, portal: 10.11.1.1,3260].  
iscsiadm: initiator reported error (8 - connection timed out)  
Login to [iface: default, target: iqn.2014-11.home.lab.san:storage01, portal: 192.168.1.5,3260] successful.  
iscsiadm: Could not log into all portals  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Makes sense that the target on &lt;code&gt;10.11.1.1&lt;/code&gt; is unavailable as there&amp;rsquo;s no route to that interface from the management network.  The target on the management interface (&lt;code&gt;192.168.1.5&lt;/code&gt;) is successful.&lt;/p&gt;

&lt;h3 id=&#34;examine-iscsi-sessions-on-the-san&#34;&gt;Examine iSCSI sessions on the SAN&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo cat /proc/net/iet/session  
tid:2 name:iqn.2014-11.home.lab.san:storage01  
        sid:1688849964925440 initiator:iqn.1993-08.org.debian:01:399510298987  
                cid:0 ip:192.168.1.134 state:active hd:none dd:none  
        sid:564049469047296 initiator:iqn.1998-01.com.vmware:esxi-56ec2846  
                cid:0 ip:10.11.1.2 state:active hd:none dd:none  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;logout-from-your-test-session&#34;&gt;Logout from your test session&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;# from a test system, 192.168.1.134
$ sudo iscsiadm -m node -U all  
Logging out of session [sid: 4, target: iqn.2014-11.home.lab.san:storage01, portal: 192.168.1.5,3260]  
Logout of [sid: 4, target: iqn.2014-11.home.lab.san:storage01, portal: 192.168.1.5,3260] successful.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;logs-ftw&#34;&gt;Logs, ftw!&lt;/h3&gt;

&lt;p&gt;All of your iSCSI interactions should show up under &lt;code&gt;/var/log/syslog&lt;/code&gt;.  This can also be utilized when troubleshooting configuration issues.&lt;/p&gt;

&lt;h1 id=&#34;nic-name-assignment&#34;&gt;NIC Name Assignment&lt;/h1&gt;

&lt;p&gt;Another issue encountered during the initial tests of the build was that the NIC would be assigned a different name based on the hardware present in the system.  Before adding the PCI video card, I&amp;rsquo;d removed the 10Gbps NIC and used a PCI-e video card to configure the UEFI.&lt;/p&gt;

&lt;p&gt;This was the hardware configuration at the time of the OS install which caused the onboard NIC, used for management, to be named &lt;code&gt;enp6s0&lt;/code&gt;.  When the PCI-e video card was replaced with the 10Gbps NIC, I quickly learned that the onboard NIC was renamed to &lt;code&gt;enp7s0&lt;/code&gt; &amp;ndash; for which no entry existed in &lt;code&gt;/etc/network/interfaces&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This is due to systemd&amp;rsquo;s approach to naming nomenclature; about which you can find more information in &lt;a href=&#34;https://askubuntu.com/a/689143&#34;&gt;this AskUbuntu post&lt;/a&gt;.  You can address this by adding &lt;a href=&#34;https://askubuntu.com/a/785442&#34;&gt;udev rules&lt;/a&gt;, but I opted for an entry to my &lt;code&gt;interfaces&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /etc/network/interfaces
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto enp7s0
iface enp7s0 inet dhcp

# The primary network interface when 10Gbps nic is not in
auto enp6s0
iface enp6s0 inet dhcp

# 10Gbps nic
auto enp1s0
iface enp1s0 inet static
  address 10.11.1.1
  # gateway 10.11.1.1
  netmask 255.255.255.0
  network 10.11.1.0
  broadcast 10.11.1.255
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;wrapping-up&#34;&gt;Wrapping Up&lt;/h1&gt;

&lt;p&gt;That completes the build of the SAN.  We will tie everything together from ESXi in the third/final post of the series.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Home Lab Overview</title>
      <link>https://int0x80.github.io/blog/post/home-lab-overview/</link>
      <pubDate>Mon, 23 Jan 2017 16:05:31 -0600</pubDate>
      
      <guid>https://int0x80.github.io/blog/post/home-lab-overview/</guid>
      <description>&lt;p&gt;Your home lab is a crucial platform for learning and advancing your skills.  From the comfort and privacy of your own home; you can spin up VMs, build infrastructure, break targets, repeatedly fail, and eventually succeed.  This post is the first in a three-part series covering my lab at home (&lt;a href=&#34;https://int0x80.github.io/blog/post/home-lab-san/&#34;&gt;Part 2&lt;/a&gt;, &lt;a href=&#34;https://int0x80.github.io/blog/post/home-lab-esxi/&#34;&gt;Part 3&lt;/a&gt;).  &lt;/p&gt;

&lt;h1 id=&#34;series-structure&#34;&gt;Series Structure&lt;/h1&gt;

&lt;p&gt;In this first post, I&amp;rsquo;ll give an overview of the lab and highlight some of the more noteworthy aspects.  In the second post, I&amp;rsquo;ll talk about the build and configuration of my SAN, which stores and serves up all of the VMs.  Finally, we&amp;rsquo;ll walk through the setup of my &lt;a href=&#34;https://www.vmware.com/products/vsphere-hypervisor.html&#34;&gt;ESXi&lt;/a&gt; node, which manages and controls the VMs.&lt;/p&gt;

&lt;h1 id=&#34;humble-beginnings&#34;&gt;Humble Beginnings&lt;/h1&gt;

&lt;p&gt;I want to take a step back to discuss the early days of my lab.  Prepare for one of those &amp;ldquo;back in my day&amp;rdquo; flashbacks, or skip to the next section to avoid it.&lt;/p&gt;

&lt;p&gt;Before virtualization, I used to hoard old hardware that was free.  Money was scarce and electricity was cheap/free.  I volunteered at several local organizations that accepted systems donated from local businesses, then filled orders for non-profit organizations who needed computers.  Occasionally the bottom-tier of inventory would need be cleared out to make room for more modern donations, and occasionally I would be able to take one of these bottom-tier systems home with me.  We&amp;rsquo;re talking about taking home a Pentium II in the days of Pentium 4 HT.  I also once landed a Sun SPARCstation!  These things ran Linux and BSD just fine, but had no virtualization support.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/1061032/22214449/768375b6-e14c-11e6-90e7-f2efc9b4e2d7.jpg&#34; alt=&#34;Sun SPARCstation, photo from kiwigeek.com&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Most consumer desktop/laptop CPUs now provide virtualization via AMD-V or Intel VT.  In the event you&amp;rsquo;re operating from hardware without virtualization support, you can acquire inexpensive devices like the Raspberry Pi or one of the many available alternatives to use as targets.  Of course, volunteering and dumpster diving are still viable options, as well.&lt;/p&gt;

&lt;p&gt;I was hooked on virtualization once capable hardware was available to me.  Nothing large scale, as my VMs were running from a single consumer-grade system, but I could at least spin up multiple systems to do things like malware analysis or isolated browsing with snapshosts.  Snapshots are a critical feature since re-imaging a system can take many minutes to hours, while it takes less than one second to revert a snapshot and restore system state.  Bonus for also not needing extra keyboards, mice, monitors, etc. since all VMs are accessible from your host system instead of being individual towers or laptops.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;tl;dr:&lt;/em&gt; &lt;strong&gt;Opt for virtualization if possible, but inexpensive/free hardware can also be effective.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;current-state&#34;&gt;Current State&lt;/h1&gt;

&lt;p&gt;My home lab currently consists of two physical systems:  a SAN, and an ESXi node.  The configuration of each system will be covered in subsequent posts; but I&amp;rsquo;ll take this opportunity to #humblebrag on some of the stats.  The SAN exposes ~22TB of storage via 10Gbps link, while the ESXi node has two Xeon CPUs with 12 cores and 96GB of RAM.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/1061032/22214656/480d9760-e14d-11e6-855e-943be3e779f5.png&#34; alt=&#34;ESXi stats humblebrag&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Projects are separated into individual virtual networks.  For example, my VMs for firmware reversing are on one network while VMs from &lt;a href=&#34;https://www.vulnhub.com/&#34;&gt;VulnHub&lt;/a&gt; are on another network.  Each network is managed by a virtual switch and a clone of a simple router VM that I built using Debian, dnsmasq, and iptables.  Larger projects with multiple network segments use multiple switches and router VMs.&lt;/p&gt;

&lt;h1 id=&#34;wrapping-up&#34;&gt;Wrapping Up&lt;/h1&gt;

&lt;p&gt;Instantly spinning up new VMs and networks allows me to focus on the goals of a project, rather than being distracted by infrastructure tasks.  Likewise, instantly tearing down or reverting VMs curbs inhibition of a serious mistake wiping out hours or days of work.  A robust lab abstracts away many of the management items thus reducing overhead and facilitating efficiency in starting and completing new projects.&lt;/p&gt;

&lt;p&gt;In the next part, we&amp;rsquo;ll cover the build and configuration of the SAN, which was quite a fun learning experience.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Blog Notes</title>
      <link>https://int0x80.github.io/blog/post/blog-notes/</link>
      <pubDate>Sat, 21 Jan 2017 08:50:47 -0600</pubDate>
      
      <guid>https://int0x80.github.io/blog/post/blog-notes/</guid>
      <description>&lt;p&gt;Hello, World.  Figured it was apropos to start the blog with some notes on how the blog was built.  Read on for details of standing up a Hugo blog on GitHub Pages.&lt;/p&gt;

&lt;h1 id=&#34;install&#34;&gt;Install&lt;/h1&gt;

&lt;p&gt;The blog is written and published from an &lt;a href=&#34;https://github.com/int0x80/ctf-vm&#34;&gt;Ubuntu VM&lt;/a&gt;.  A build of hugo is already pre-packaged and available in apt, but unfortunately ships with some bugs.  Fear not, installing the &lt;a href=&#34;https://github.com/spf13/hugo/releases/latest&#34;&gt;latest release&lt;/a&gt; is simple.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://github.com/spf13/hugo/releases/download/v0.18.1/hugo_0.18.1-64bit.deb
$ sudo dpkg -i ./hugo_0.18.1-64bit.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A quick version check confirms the install.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo version 
Hugo Static Site Generator v0.18.1 BuildDate: 2016-12-30T04:05:34-06:00
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;new-site-new-post&#34;&gt;New Site, New Post&lt;/h1&gt;

&lt;p&gt;Getting the blog off the ground takes only a moment.  First create a site, then a post.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo new site blog
$ cd !$ 
$ hugo new post/blog-notes.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Posts are written in markdown so editing is quick, simple, and can be accomplished with your editor of choice.&lt;/p&gt;

&lt;h1 id=&#34;what-s-important&#34;&gt;What&amp;rsquo;s Important&lt;/h1&gt;

&lt;p&gt;Aside from publishing via static page generator, I also had a few demands.  Syntax highlighting should look nice, and be easily integrated.  For example, I prefer markdown&amp;rsquo;s wrapping as opposed to MediaWiki&amp;rsquo;s GeSHi &lt;code&gt;&amp;lt;syntaxhighlight ...&amp;gt;&lt;/code&gt; delimeters.  Next, a presentable theme that can easily be wired up.  And lastly, customized summaries to keep the listing pages tidy.&lt;/p&gt;

&lt;h2 id=&#34;syntax-highlighting&#34;&gt;Syntax Highlighting&lt;/h2&gt;

&lt;p&gt;Hugo uses &lt;a href=&#34;http://pygments.org/&#34;&gt;Pygments&lt;/a&gt; for syntax highlighting.  A client-side highlighter, like &lt;a href=&#34;https://highlightjs.org/&#34;&gt;highlight.js&lt;/a&gt; can easily be incorporated.  I opted for the latter based on the availability of languages and styles.  Lexers and styles are easily incorporated in the header of each post.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scripts = [
  &amp;quot;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js&amp;quot;,
]

css = [
  &amp;quot;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/ocean.min.css&amp;quot;,
]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;theme&#34;&gt;Theme&lt;/h2&gt;

&lt;p&gt;As with any modern blogging app, Hugo facilitates &lt;a href=&#34;http://themes.gohugo.io/&#34;&gt;themes&lt;/a&gt;.  I went with &lt;a href=&#34;http://themes.gohugo.io/hugo-geo/&#34;&gt;Hugo Geo&lt;/a&gt; and pretty much copied the stylesheet from author&amp;rsquo;s site.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd themes/
$ git clone https://github.com/alexurquhart/hugo-geo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then configure the &lt;code&gt;config.toml&lt;/code&gt; file in the main directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;baseURL = &amp;quot;https://int0x80.github.io/blog/&amp;quot;
languageCode = &amp;quot;en-us&amp;quot;
title = &amp;quot;Hack All The Things&amp;quot;
disqusShortname = &amp;quot;xcdx80&amp;quot;
paginate = 5


[taxonomies]
  tag = &amp;quot;tags&amp;quot;
  category = &amp;quot;categories&amp;quot;
  series = &amp;quot;series&amp;quot;

[params]
  showglobe = false
  highlight = true

  favicon = &amp;quot;favicon.ico&amp;quot;

  github = &amp;quot;https://github.com/int0x80&amp;quot;
  twitter = &amp;quot;https://twitter.com/int0x80&amp;quot;
  
  profilepic = &amp;quot;img/int0x80.jpg&amp;quot;
  
  title = &amp;quot;int0x80&amp;quot;
  subtitle = &amp;quot;Hack All The Things&amp;quot;
  
  [[params.navlinks]]
  name = &amp;quot;About&amp;quot;
  url = &amp;quot;about&amp;quot;
  
  [[params.navlinks]]
  name = &amp;quot;Blog&amp;quot;
  url = &amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary-length&#34;&gt;Summary Length&lt;/h2&gt;

&lt;p&gt;The default length for a post&amp;rsquo;s summary is 70 words.  Meaning Hugo displays the first 70 words of a post, followed by a &amp;ldquo;Read More&amp;rdquo; link to the full post.  This can be modified by inserting &lt;code&gt;&amp;lt;!--more--&amp;gt;&lt;/code&gt; into the post content to delimit the summary.  As an example, the beginning content of this post is as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Hello, World.  Figured it was apropos to start the blog with some notes on how the blog was built.  Read on for details of standing up a Hugo blog on GitHub Pages.&amp;lt;!--more--&amp;gt;

# Install

The blog is written and published from an [Ubuntu VM](https://github.com/int0x80/ctf-vm).  Fortunately a build of hugo is already pre-packaged and available in apt.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;github-pages&#34;&gt;GitHub Pages&lt;/h1&gt;

&lt;p&gt;The last steps are to do some git wrangling to push the site up to GitHub.  The &lt;code&gt;baseURL&lt;/code&gt; parameter has already been situated in the config so let&amp;rsquo;s generate the site content with hugo.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo --theme=hugo-geo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This creates the &lt;code&gt;public/&lt;/code&gt; directory which will be pushed up to GitHub.  Now to set up version control.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git init
$ echo &amp;quot;/public/&amp;quot; &amp;gt;&amp;gt; .gitignore
$ echo &amp;quot;/themes/&amp;quot; &amp;gt;&amp;gt; .gitignore
$ git add --all
$ git commit -m &amp;quot;Initial commit&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, create an empty repository named &lt;code&gt;blog&lt;/code&gt; on GitHub; and add it as remote.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd public
$ git init
$ git remote add origin git@github.com:int0x80/blog.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then create a branch and push it up.  Remember, we&amp;rsquo;re still under &lt;code&gt;public/&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git checkout -b gh-pages
$ git add --all
$ git commit -m &amp;quot;blog added&amp;quot;
$ git push -f origin gh-pages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mostly painless, overall.  I look forward to sharing my notes with the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>https://int0x80.github.io/blog/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://int0x80.github.io/blog/about/</guid>
      <description>&lt;p&gt;My name is int0x80 (pronounced &amp;ldquo;int eighty&amp;rdquo;), I&amp;rsquo;m the rapper in &lt;a href=&#34;http://dualcoremusic.com&#34;&gt;Dual Core&lt;/a&gt;.  My main interests are security, reverse engineering, and privacy.  Mess around with technology and see what happens.  Hack all the things!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>